#!/usr/bin/env python3
"""
clone_manager.py - GitHub ä»“åº“å…‹éš†ç®¡ç†å™¨
-----------------------------------------
è´Ÿè´£ä» GitHub å…‹éš†æŠ€èƒ½ä»“åº“åˆ°æœ¬åœ°æš‚å­˜ç©ºé—´

èŒè´£:
1. GitHub ä»“åº“å…‹éš†ï¼ˆæ”¯æŒåŠ é€Ÿå™¨ï¼‰
2. è¿œç¨‹ä»“åº“åˆ†æï¼ˆé¢„æ£€ã€ç¼“å­˜ï¼‰
3. æŠ€èƒ½ç›®å½•æå–
4. ä»“åº“ç¼“å­˜ç®¡ç†

Architecture:
    skill_manager â†’ clone_manager â†’ security_scanner
                       â†“
                  æš‚å­˜ç©ºé—´ (mybox/cache/repos/)

Source: Refactored from skill_manager.py (Apache 2.0)
"""

import argparse
import json
import os
import re
import shutil
import subprocess
import sys
import tempfile
import time
import zipfile
from concurrent.futures import ThreadPoolExecutor
from contextlib import contextmanager
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
from urllib.parse import urlparse
import yaml

# =============================================================================
# è·¯å¾„é…ç½®
# =============================================================================

BASE_DIR = Path(__file__).parent.parent
CACHE_DIR = BASE_DIR / "mybox" / "cache" / "repos"
TEMP_DIR = BASE_DIR / "mybox" / "temp"

# æŠ€èƒ½é»˜è®¤å€¼å¸¸é‡
DEFAULT_SKILL_DESC = "æ— æè¿°"
DEFAULT_SKILL_CATEGORY = "utilities"
DEFAULT_SKILL_TAGS = ["skill"]

# æ·»åŠ é¡¹ç›® lib ç›®å½•åˆ° sys.pathï¼ˆç»¿è‰²åŒ…é¢„ç½®ä¾èµ–ï¼‰
_lib_dir = Path(__file__).parent.parent / "lib"
if _lib_dir.exists():
    sys.path.insert(0, str(_lib_dir))

# æ·»åŠ  bin ç›®å½•åˆ° sys.pathï¼ˆç”¨äºå¯¼å…¥å…¶ä»–æ¨¡å—ï¼‰
_bin_dir = Path(__file__).parent
if str(_bin_dir) not in sys.path:
    sys.path.insert(0, str(_bin_dir))

# =============================================================================
# æ—¥å¿—å·¥å…·
# =============================================================================

def log(level: str, message: str, emoji: str = ""):
    """ç»Ÿä¸€çš„æ—¥å¿—è¾“å‡º"""
    timestamp = datetime.now().strftime("%H:%M:%S")
    print(f"{timestamp} [{level}] {emoji} {message}")

def success(msg: str):
    log("SUCCESS", msg, "âœ…")

def info(msg: str):
    log("INFO", msg, "ğŸ”„")

def warn(msg: str):
    log("WARN", msg, "âš ï¸")

def error(msg: str):
    log("ERROR", msg, "âŒ")

# =============================================================================
# é…ç½®åŠ è½½ï¼ˆå…±äº«ï¼‰
# =============================================================================

_config_cache: Optional[dict] = None
_config_mtime: Optional[float] = None

def load_config(use_cache: bool = True) -> dict:
    """
    åŠ è½½é…ç½®æ–‡ä»¶ï¼ˆæ”¯æŒç¼“å­˜ï¼‰

    é…ç½®æ–‡ä»¶ä¼˜å…ˆçº§ï¼ˆæŒ‰é¡ºåºï¼‰ï¼š
    1. <BASE_DIR>/config.jsonï¼ˆæ¨èï¼ŒJSONæ ¼å¼ï¼‰
    2. <BASE_DIR>/.claude/config/config.ymlï¼ˆå‘åå…¼å®¹ï¼ŒYAMLæ ¼å¼ï¼‰

    Args:
        use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜ï¼ˆé»˜è®¤ Trueï¼‰

    Returns:
        é…ç½®å­—å…¸
    """
    global _config_cache, _config_mtime

    # é…ç½®æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰
    config_files = [
        BASE_DIR / "config.json",
        BASE_DIR / ".claude" / "config" / "config.yml"
    ]

    # æŸ¥æ‰¾å­˜åœ¨çš„é…ç½®æ–‡ä»¶
    config_file = None
    for cf in config_files:
        if cf.exists():
            config_file = cf
            break

    if not config_file:
        return {}

    # æ£€æŸ¥ç¼“å­˜æœ‰æ•ˆæ€§
    if use_cache and _config_cache is not None:
        current_mtime = config_file.stat().st_mtime
        if current_mtime == _config_mtime:
            return _config_cache

    # åŠ è½½é…ç½®ï¼ˆæ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©åŠ è½½æ–¹å¼ï¼‰
    try:
        with open(config_file, "r", encoding="utf-8") as f:
            if config_file.suffix == ".json":
                _config_cache = json.load(f)
            else:
                _config_cache = yaml.safe_load(f) or {}
            _config_mtime = config_file.stat().st_mtime
            return _config_cache
    except Exception as e:
        warn(f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
        return {}

def clear_config_cache() -> None:
    """æ¸…é™¤é…ç½®ç¼“å­˜"""
    global _config_cache, _config_mtime
    _config_cache = None
    _config_mtime = None

def get_git_proxies() -> list:
    """è·å– Git åŠ é€Ÿå™¨åˆ—è¡¨"""
    config = load_config()
    return config.get("git", {}).get("proxies", [
        "https://ghp.ci/{repo}",
        "https://ghproxy.net/{repo}",
    ])

def get_ssl_verify() -> bool:
    """è·å– SSL éªŒè¯è®¾ç½®"""
    config = load_config()
    return config.get("git", {}).get("ssl_verify", True)

def get_raw_proxies() -> list:
    """è·å– Raw URL åŠ é€Ÿå™¨åˆ—è¡¨"""
    config = load_config()
    return config.get("raw", {}).get("proxies", [
        "https://ghp.ci/{path}",
        "https://raw.fastgit.org/{path}",
    ])

# =============================================================================
# æ ¼å¼æ£€æµ‹å™¨
# =============================================================================

class FormatDetector:
    """æ£€æµ‹è¾“å…¥æºçš„æ ¼å¼ç±»å‹"""

    @staticmethod
    def validate_github_url(url: str) -> Tuple[bool, Optional[str]]:
        """
        éªŒè¯ GitHub URL æ ¼å¼å®‰å…¨æ€§ï¼Œé˜²æ­¢é…ç½®æ³¨å…¥æ”»å‡»

        Args:
            url: å¾…éªŒè¯çš„ GitHub URL

        Returns:
            (æ˜¯å¦æœ‰æ•ˆ, é”™è¯¯ä¿¡æ¯)
        """
        # åŸºç¡€æ ¼å¼æ£€æŸ¥
        github_pattern = r'^https?://github\.com/[a-zA-Z0-9_-]+/[a-zA-Z0-9_.-]+(?:/tree/[^/\s]+(?:/[\w\-./]+)?)?$'
        if not re.match(github_pattern, url):
            return False, f"æ— æ•ˆçš„ GitHub URL æ ¼å¼: {url}"

        # æ£€æŸ¥å±é™©çš„ git é…ç½®æ³¨å…¥æ¨¡å¼
        dangerous_patterns = [
            '--config=', '-c=', '--upload-pack=', '--receive-pack=',
            '--exec=', '&&', '||', '|', '`', '$(', '\n', '\r', '\x00',
        ]

        url_lower = url.lower()
        for pattern in dangerous_patterns:
            if pattern in url_lower:
                return False, f"URL åŒ…å«å±é™©å­—ç¬¦æˆ–æ¨¡å¼: {pattern}"

        # æ£€æŸ¥ URL ç¼–ç ç»•è¿‡å°è¯•
        if '%2' in url.lower():
            return False, "URL åŒ…å«å¯ç–‘çš„ç¼–ç å­—ç¬¦"

        return True, None

    @staticmethod
    def parse_github_subpath(github_url: str) -> Tuple[str, Optional[str]]:
        """
        è§£æ GitHub URLï¼Œæå–å­è·¯å¾„

        Returns:
            (ä»“åº“URL, å­è·¯å¾„)
        """
        is_valid, error_msg = FormatDetector.validate_github_url(github_url)
        if not is_valid:
            raise ValueError(f"GitHub URL å®‰å…¨éªŒè¯å¤±è´¥: {error_msg}")

        parsed = urlparse(github_url)
        path_parts = parsed.path.strip('/').split('/')

        # æŸ¥æ‰¾ /tree/ åˆ†éš”ç¬¦
        if 'tree' in path_parts:
            tree_idx = path_parts.index('tree')
            if tree_idx >= 2:
                repo_url = f"{parsed.scheme}://{parsed.netloc}/{'/'.join(path_parts[:tree_idx])}"
                if len(path_parts) > tree_idx + 2:
                    subpath = '/'.join(path_parts[tree_idx + 2:])
                    return repo_url, subpath
                return repo_url, None

        return github_url, None

    @staticmethod
    def detect_input_type(input_source: str) -> Tuple[str, str, Optional[str]]:
        """
        æ£€æµ‹è¾“å…¥æºç±»å‹

        Returns:
            (ç±»å‹, è·¯å¾„/URL, å­è·¯å¾„)
        """
        info(f"æ£€æµ‹è¾“å…¥æº: {input_source}")

        # 1. æ£€æŸ¥æ˜¯å¦æ˜¯ GitHub URL
        if input_source.startswith(("http://", "https://")):
            parsed = urlparse(input_source)
            if "github.com" in parsed.netloc:
                repo_url, subpath = FormatDetector.parse_github_subpath(input_source)
                if subpath:
                    info(f"æ£€æµ‹åˆ°å­è·¯å¾„: {subpath}")
                return "github", repo_url, subpath

        # 1.5 æ£€æŸ¥æ˜¯å¦æ˜¯ GitHub ç®€å†™ (user/repo)
        if "/" in input_source and not input_source.startswith((".", "/", "\\")):
            parts = input_source.split("/")
            if len(parts) == 2 and not any(c in input_source for c in ":\\"):
                return "github", f"https://github.com/{input_source}", None

        # 2. æ£€æŸ¥æ˜¯å¦æ˜¯æœ¬åœ°è·¯å¾„
        local_path = Path(input_source).expanduser().resolve()
        if local_path.exists():
            if local_path.is_file() and local_path.suffix == ".skill":
                return "skill-package", str(local_path), None
            elif local_path.is_dir():
                return "local", str(local_path), None

        # 3. æ£€æŸ¥æ˜¯å¦æ˜¯ç›¸å¯¹è·¯å¾„
        relative_path = BASE_DIR / input_source
        if relative_path.exists():
            if relative_path.is_file() and relative_path.suffix == ".skill":
                return "skill-package", str(relative_path), None
            elif relative_path.is_dir():
                return "local", str(relative_path), None

        warn("æ— æ³•è¯†åˆ«è¾“å…¥æºç±»å‹ï¼Œå°è¯•ä½œä¸ºæœ¬åœ°ç›®å½•å¤„ç†")
        return "unknown", input_source, None

# =============================================================================
# æŠ€èƒ½æ ‡å‡†åŒ–å™¨ï¼ˆæå–æ‰€éœ€éƒ¨åˆ†ï¼‰
# =============================================================================

class SkillNormalizer:
    """å°†å„ç§æ ¼å¼æ ‡å‡†åŒ–ä¸ºå®˜æ–¹ SKILL.md æ ¼å¼"""

    @staticmethod
    def extract_frontmatter(content: str) -> Dict[str, Any]:
        """ä» SKILL.md æå– YAML frontmatter"""
        if not content.startswith("---"):
            return {}

        end_marker = content.find("\n---", 4)
        if end_marker == -1:
            end_marker = content.find("---", 3)
        if end_marker <= 0:
            return {}

        yaml_content = content[4:end_marker].strip()

        try:
            frontmatter = yaml.safe_load(yaml_content)
            if isinstance(frontmatter, dict):
                return frontmatter
        except (yaml.YAMLError, Exception):
            pass

        # é™çº§ï¼šæ‰‹åŠ¨è§£æ
        result = {}
        for line in yaml_content.split('\n'):
            if ':' in line and not line.strip().startswith('#'):
                key, value = line.split(':', 1)
                result[key.strip()] = value.strip().strip('"').strip("'")
        return result

# =============================================================================
# é¡¹ç›®éªŒè¯å™¨
# =============================================================================

class ProjectValidator:
    """éªŒè¯é¡¹ç›®æ˜¯å¦ä¸ºæŠ€èƒ½ä»“åº“"""

    TOOL_PROJECT_FILES = [
        "setup.py", "Cargo.toml", "go.mod",
    ]

    AMBIGUOUS_PROJECT_FILES = [
        "pyproject.toml", "package.json",
    ]

# =============================================================================
# æŠ€èƒ½åŒ…å¤„ç†å™¨
# =============================================================================

class SkillPackHandler:
    """å¤„ç† .skill æŠ€èƒ½åŒ…"""

    @staticmethod
    def extract_pack(pack_file: Path, extract_dir: Path) -> Tuple[bool, Optional[Path]]:
        """
        è§£å‹ .skill æŠ€èƒ½åŒ…

        Returns:
            (æˆåŠŸ, è§£å‹ç›®å½•)
        """
        try:
            with zipfile.ZipFile(pack_file, 'r') as zip_ref:
                zip_ref.extractall(extract_dir)
            return True, extract_dir
        except Exception as e:
            error(f"è§£å‹æŠ€èƒ½åŒ…å¤±è´¥: {e}")
            return False, None

# =============================================================================
# è¿œç¨‹æŠ€èƒ½åˆ†æå™¨
# =============================================================================

class RemoteSkillAnalyzer:
    """åˆ†æè¿œç¨‹ GitHub ä»“åº“çš„æŠ€èƒ½ä¿¡æ¯"""

    API_BASE = "https://api.github.com"
    RAW_BASE = "https://raw.githubusercontent.com"

    def __init__(self, repo: str, branch: str = "main", token: Optional[str] = None):
        """
        Args:
            repo: user/repo æ ¼å¼
            branch: åˆ†æ”¯å
            token: GitHub personal access token
        """
        self.repo = repo
        self.branch = branch
        self.token = token or os.environ.get("GITHUB_TOKEN")
        self._use_cache = True
        self.github_url = f"https://github.com/{repo}"
        self.proxies = get_raw_proxies()
        self._cache = {}
        self._working_proxy = None

    def analyze(self) -> Dict[str, Any]:
        """åˆ†æä»“åº“ï¼Œè¿”å›æŠ€èƒ½ä¿¡æ¯"""
        result = {
            "repo": self.repo,
            "branch": "main",
            "skills": [],
            "source": "unknown"
        }

        # æ£€æŸ¥ç¼“å­˜
        cache_dir = RepoCacheManager._get_cache_dir(self.github_url)
        if cache_dir.exists() and self._use_cache:
            meta = RepoCacheManager.load_meta(cache_dir)
            if meta and meta.get("url") == self.github_url:
                result["source"] = "cache"
                info(f"ä½¿ç”¨æœ¬åœ°ç¼“å­˜åˆ†æ: {self.repo}")
                return self._analyze_from_cache(cache_dir, result)

        # ç½‘ç»œæ¢æµ‹
        result["source"] = "network"
        return self._analyze_from_network(result)

    def _analyze_from_cache(self, cache_dir: Path, result: Dict) -> Dict:
        """ä»æœ¬åœ°ç¼“å­˜åˆ†æä»“åº“"""
        skills = []
        for skill_md in cache_dir.rglob("SKILL.md"):
            rel_path = skill_md.relative_to(cache_dir)
            try:
                with open(skill_md, "r", encoding="utf-8", errors="ignore") as f:
                    content = f.read()
                info = self._parse_skill_md(content, str(rel_path.parent))
                if info:
                    info["url"] = f"{self.github_url}/tree/{result['branch']}/{rel_path.parent}"
                    info["is_root"] = (str(rel_path.parent) == ".")
                    skills.append(info)
            except Exception as e:
                warn(f"è¯»å– {rel_path} å¤±è´¥: {e}")

        result["skills"] = sorted(skills, key=lambda x: x["name"])
        return result

    def _analyze_from_network(self, result: Dict) -> Dict:
        """é€šè¿‡ç½‘ç»œåˆ†æä»“åº“"""
        skills = []

        # æ£€æµ‹åˆ†æ”¯
        branches_to_try = ["main", "master"]
        found_branch = None

        for branch in branches_to_try:
            self.branch = branch
            if self.fetch_file("SKILL.md") or self.fetch_file("README.md"):
                found_branch = branch
                break

        if not found_branch:
            return result

        result["branch"] = found_branch

        # æ£€æµ‹æ ¹ç›®å½• SKILL.md
        root_skill_content = self.fetch_file("SKILL.md")
        if root_skill_content:
            root_info = self._parse_skill_md(root_skill_content, "")
            if root_info:
                root_info["is_root"] = True
                root_info["url"] = self.github_url
                skills.append(root_info)

        # æ¢æµ‹å­æŠ€èƒ½
        sub_skill_paths = self._discover_skill_paths()
        for path in sub_skill_paths:
            content = self.fetch_file(path)
            if content:
                info = self._parse_skill_md(content, path)
                if info:
                    folder = path.rsplit("/", 1)[0] if "/" in path else ""
                    info["url"] = f"{self.github_url}/tree/{found_branch}/{folder}"
                    info["is_root"] = False
                    skills.append(info)

        result["skills"] = sorted(skills, key=lambda x: x["name"])
        return result

    def _discover_skill_paths(self) -> List[str]:
        """æ¢æµ‹å­æŠ€èƒ½ SKILL.md è·¯å¾„"""
        skill_paths = []
        checked = set()

        # å¸¸è§æŠ€èƒ½åç§°
        common_names = [
            "commit", "review-pr", "pdf", "web-search", "image-analysis",
            "doc-coauthoring", "copywriting", "email-sequence", "popup-cro",
            "translator", "summarizer", "code-run", "terminal"
        ]

        patterns = [
            "plugins/{name}/SKILL.md",
            "skills/{name}/SKILL.md",
            "{name}/SKILL.md",
        ]

        names_to_check = common_names

        for pattern in patterns:
            for name in names_to_check:
                path = pattern.replace("{name}", name)
                if path in checked:
                    continue
                if self.fetch_file(path):
                    skill_paths.append(path)
                    checked.add(path)

        return sorted(skill_paths)

    def _parse_skill_md(self, content: str, file_path: str) -> Optional[Dict]:
        """è§£æ SKILL.md å†…å®¹"""
        frontmatter = SkillNormalizer.extract_frontmatter(content)

        name = frontmatter.get("name", "")
        if not name:
            if file_path:
                folder = file_path.replace("\\", "/").rsplit("/", 1)[-1] if "/" in file_path else ""
                name = folder if folder else "unknown"
            else:
                name = "unknown"

        return {
            "name": name,
            "folder": file_path.replace("\\", "/") if file_path else "",
            "description": frontmatter.get("description", DEFAULT_SKILL_DESC),
            "category": frontmatter.get("category", DEFAULT_SKILL_CATEGORY),
            "tags": frontmatter.get("tags", DEFAULT_SKILL_TAGS.copy())
        }

    @staticmethod
    def _validate_url(url: str) -> bool:
        """éªŒè¯ URL å®‰å…¨æ€§"""
        try:
            parsed = urlparse(url)
            if parsed.scheme not in ('http', 'https'):
                return False
            if not parsed.netloc:
                return False
            dangerous = [';', '&', '|', '$', '`', '\n', '\r']
            return not any(c in url for c in dangerous)
        except Exception:
            return False

    def fetch_file(self, file_path: str, prefer_api: bool = False) -> Optional[str]:
        """è·å–æ–‡ä»¶å†…å®¹ - è‡ªåŠ¨é€‰æ‹©æœ€ä½³æ–¹å¼"""
        if file_path in self._cache:
            return self._cache[file_path]

        if prefer_api and self.token:
            content = self._fetch_via_api(file_path)
            if content is not None:
                self._cache[file_path] = content
                return content

        content = self._fetch_via_raw(file_path)
        if content is not None:
            self._cache[file_path] = content
            return content

        if self.token and not prefer_api:
            content = self._fetch_via_api(file_path)
            if content is not None:
                self._cache[file_path] = content
                return content

        return None

    def _fetch_via_raw(self, file_path: str) -> Optional[str]:
        """é€šè¿‡ Raw URL è·å–æ–‡ä»¶"""
        path = f"{self.repo}/{self.branch}/{file_path}"

        if self._working_proxy:
            proxy_url = self._working_proxy.replace("{path}", path)
            content = self._try_fetch_url(proxy_url)
            if content is not None:
                return content
            self._working_proxy = None

        for proxy_template in self.proxies:
            proxy_url = proxy_template.replace("{path}", path)
            content = self._try_fetch_url(proxy_url)
            if content is not None:
                self._working_proxy = proxy_template
                return content

        raw_url = f"{self.RAW_BASE}/{path}"
        return self._try_fetch_url(raw_url)

    def _try_fetch_url(self, url: str) -> Optional[str]:
        """å°è¯•ä»æŒ‡å®š URL è·å–æ–‡ä»¶"""
        if not self._validate_url(url):
            return None
        try:
            result = subprocess.run(
                ["curl", "-s", "--max-time", "5", "--connect-timeout", "3", url],
                capture_output=True,
                text=True,
                encoding="utf-8",
                errors="replace",
                check=False
            )
            if result.returncode == 0 and result.stdout:
                content = result.stdout
                if not content or "404: Not Found" in content or "<title>404" in content:
                    return None
                return content
        except Exception:
            pass
        return None

    def _fetch_via_api(self, file_path: str) -> Optional[str]:
        """é€šè¿‡ GitHub API è·å–æ–‡ä»¶"""
        url = f"{self.API_BASE}/repos/{self.repo}/contents/{file_path}"
        headers = {"Accept": "application/vnd.github.v3.raw"}
        if self.token:
            headers["Authorization"] = f"token {self.token}"

        cmd = ["curl", "-s", "--max-time", "3", url]
        for k, v in headers.items():
            cmd.extend(["-H", f"{k}: {v}"])

        try:
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                encoding="utf-8",
                errors="replace",
                check=False
            )

            if result.returncode == 403:
                return None

            if result.returncode == 200 and result.stdout:
                if result.stdout.startswith("{"):
                    try:
                        import base64
                        data = json.loads(result.stdout)
                        if "content" in data:
                            return base64.b64decode(data["content"]).decode("utf-8", errors="ignore")
                    except Exception:
                        pass
                return result.stdout
            return None
        except Exception:
            return None

    def check_is_skill_repo(self) -> Tuple[Optional[bool], str]:
        """ç»Ÿä¸€çš„æŠ€èƒ½ä»“åº“é¢„æ£€"""
        # å°è¯• Raw URL
        content = self.fetch_file("SKILL.md")
        if content:
            return True, f"Raw å‘ç° SKILL.md (åˆ†æ”¯: {self.branch})"

        if self.fetch_file("skills/commit/SKILL.md"):
            return True, f"Raw å‘ç° skills/ ç›®å½• (åˆ†æ”¯: {self.branch})"

        return None, "é¢„æ£€è¶…æ—¶æˆ–å¤±è´¥ï¼Œé™çº§åˆ° clone"

    def _verify_single_skill(self, skill_name: str) -> bool:
        """è½»é‡çº§éªŒè¯ï¼šä»…æ£€æŸ¥æŒ‡å®šæŠ€èƒ½æ˜¯å¦å­˜åœ¨"""
        normalized = skill_name.lower().replace('_', '-')

        patterns = [
            f"skills/{skill_name}/SKILL.md",
            f"plugins/{skill_name}/SKILL.md",
            f"{skill_name}/SKILL.md",
        ]

        if '-' in skill_name or '_' in skill_name:
            alt_patterns = [
                f"skills/{normalized}/SKILL.md",
                f"plugins/{normalized}/SKILL.md",
                f"{normalized}/SKILL.md",
            ]
            patterns.extend(alt_patterns)

        for pattern in patterns:
            if self.fetch_file(pattern):
                return True

        return False

# =============================================================================
# ä»“åº“ç¼“å­˜ç®¡ç†å™¨
# =============================================================================

class RepoCacheManager:
    """ç®¡ç† GitHub ä»“åº“çš„æŒä¹…åŒ–ç¼“å­˜"""

    @staticmethod
    def _sanitize_url(url: str) -> str:
        """å°† URL è½¬æ¢ä¸ºå®‰å…¨çš„ç›®å½•å"""
        clean = url.replace("https://", "").replace("http://", "")
        clean = clean.replace("/", "_").replace("\\", "_")
        return clean[:100]

    @staticmethod
    def _get_cache_dir(github_url: str) -> Path:
        """è·å–ä»“åº“çš„ç¼“å­˜ç›®å½•"""
        cache_name = RepoCacheManager._sanitize_url(github_url)
        return CACHE_DIR / cache_name

    @staticmethod
    def _get_meta_file(cache_dir: Path) -> Path:
        """è·å–ç¼“å­˜å…ƒæ•°æ®æ–‡ä»¶è·¯å¾„"""
        return cache_dir / ".meta.json"

    @staticmethod
    def load_meta(cache_dir: Path) -> Optional[Dict]:
        """åŠ è½½ç¼“å­˜å…ƒæ•°æ®"""
        meta_file = RepoCacheManager._get_meta_file(cache_dir)
        if meta_file.exists():
            try:
                with open(meta_file, "r", encoding="utf-8") as f:
                    return json.load(f)
            except Exception:
                pass
        return None

    @staticmethod
    def save_meta(cache_dir: Path, meta: Dict):
        """ä¿å­˜ç¼“å­˜å…ƒæ•°æ®"""
        meta_file = RepoCacheManager._get_meta_file(cache_dir)
        cache_dir.mkdir(parents=True, exist_ok=True)
        with open(meta_file, "w", encoding="utf-8") as f:
            json.dump(meta, f, indent=2, ensure_ascii=False)

    @staticmethod
    def get_or_clone(
        github_url: str,
        force_refresh: bool = False,
        timeout: int = 300
    ) -> Tuple[bool, Optional[Path], str]:
        """
        è·å–ä»“åº“ï¼ˆä¼˜å…ˆä»ç¼“å­˜ï¼‰

        Returns:
            (æˆåŠŸ, ä»“åº“è·¯å¾„, æ¶ˆæ¯)
        """
        cache_dir = RepoCacheManager._get_cache_dir(github_url)

        # 1. æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨
        if cache_dir.exists() and not force_refresh:
            meta = RepoCacheManager.load_meta(cache_dir)
            if meta and meta.get("url") == github_url:
                cached_time = meta.get("cached_at", "")
                return True, cache_dir, f"ä½¿ç”¨ç¼“å­˜ (ç¼“å­˜äº {cached_time})"

        # 2. ç¼“å­˜ä¸å­˜åœ¨æˆ–å¼ºåˆ¶åˆ·æ–°ï¼Œæ‰§è¡Œå…‹éš†
        info(f"å…‹éš†ä»“åº“åˆ°ç¼“å­˜: {github_url}")

        # å¦‚æœæ—§ç¼“å­˜å­˜åœ¨ï¼Œå…ˆåˆ é™¤
        if cache_dir.exists():
            try:
                subprocess.run(["rm", "-rf", str(cache_dir)], capture_output=True, timeout=10)
            except:
                pass
            if cache_dir.exists():
                warn(f"ç¼“å­˜æ¸…ç†å¤±è´¥ï¼Œä½¿ç”¨ shutil å¼ºåˆ¶é‡è¯•: {cache_dir}")
                time.sleep(0.5)
                try:
                    shutil.rmtree(cache_dir, ignore_errors=False)
                except Exception as e:
                    error(f"æ— æ³•åˆ é™¤ç¼“å­˜ç›®å½•ï¼Œè¯·æ‰‹åŠ¨åˆ é™¤åé‡è¯•: {cache_dir}")
                    error(f"é”™è¯¯ä¿¡æ¯: {e}")
                    return False, None, f"ç¼“å­˜æ¸…ç†å¤±è´¥: {e}"

        CACHE_DIR.mkdir(parents=True, exist_ok=True)

        # æ‰§è¡Œå…‹éš†
        clone_ok, _ = GitHubHandler.clone_repo(github_url, cache_dir)

        if not clone_ok:
            return False, None, "ä»“åº“ä¸å­˜åœ¨æˆ–è·¯å¾„é”™è¯¯ï¼Œè¯·ç¡®è®¤:\n1. ä»“åº“ URL æ˜¯å¦æ­£ç¡®\n2. æ˜¯å¦ä¸ºå­æŠ€èƒ½ï¼ˆå°è¯•ä½¿ç”¨ --skill å‚æ•°ï¼‰\n3. æŸ¥çœ‹æ˜ å°„è¡¨: docs/skills-mapping.md"

        # ä¿å­˜å…ƒæ•°æ®
        meta = {
            "url": github_url,
            "cached_at": datetime.now().isoformat(),
            "branch": "main"
        }
        RepoCacheManager.save_meta(cache_dir, meta)

        return True, cache_dir, "ç¼“å­˜åˆ›å»ºæˆåŠŸ"

    @staticmethod
    def list_cache() -> List[Dict]:
        """
        åˆ—å‡ºæ‰€æœ‰ç¼“å­˜

        Returns:
            ç¼“å­˜ä¿¡æ¯åˆ—è¡¨
        """
        caches = []
        if not CACHE_DIR.exists():
            return caches

        for cache_dir in CACHE_DIR.iterdir():
            if not cache_dir.is_dir():
                continue

            meta = RepoCacheManager.load_meta(cache_dir)
            if meta:
                # è®¡ç®—ç¼“å­˜å¤§å°
                size_mb = 0
                try:
                    size = sum(f.stat().st_size for f in cache_dir.rglob('*') if f.is_file())
                    size_mb = round(size / (1024 * 1024), 2)
                except Exception:
                    pass

                caches.append({
                    "url": meta.get("url", "unknown"),
                    "cached_at": meta.get("cached_at", ""),
                    "size_mb": size_mb
                })

        return caches

    @staticmethod
    def clear_cache(older_than_hours: Optional[int] = None) -> Dict[str, int]:
        """
        æ¸…ç†ç¼“å­˜

        Args:
            older_than_hours: åªæ¸…ç†è¶…è¿‡æŒ‡å®šå°æ—¶æ•°çš„ç¼“å­˜ï¼ŒNone è¡¨ç¤ºå…¨éƒ¨æ¸…ç†

        Returns:
            {"cleared": æ¸…ç†æ•°é‡, "kept": ä¿ç•™æ•°é‡}
        """
        import time
        from datetime import datetime, timedelta

        if not CACHE_DIR.exists():
            return {"cleared": 0, "kept": 0}

        cleared = 0
        kept = 0
        now = time.time()

        for cache_dir in CACHE_DIR.iterdir():
            if not cache_dir.is_dir():
                continue

            should_delete = True

            if older_than_hours is not None:
                meta = RepoCacheManager.load_meta(cache_dir)
                if meta:
                    try:
                        cached_at = datetime.fromisoformat(meta.get("cached_at", ""))
                        age_hours = (now - cached_at.timestamp()) / 3600
                        if age_hours < older_than_hours:
                            should_delete = False
                    except Exception:
                        pass

            if should_delete:
                try:
                    shutil.rmtree(cache_dir, ignore_errors=False)
                    cleared += 1
                except Exception:
                    pass
            else:
                kept += 1

        return {"cleared": cleared, "kept": kept}

# =============================================================================
# GitHub å¤„ç†å™¨
# =============================================================================

class GitHubHandler:
    """å¤„ç† GitHub ä»“åº“çš„å…‹éš†å’Œæå–"""

    @staticmethod
    def clone_repo(github_url: str, target_dir: Path) -> Tuple[bool, Path]:
        """
        å…‹éš† GitHub ä»“åº“ï¼ˆæ”¯æŒåŠ é€Ÿå™¨ï¼‰

        Returns:
            (æˆåŠŸ, å…‹éš†ç›®å½•)
        """
        info(f"å…‹éš†ä»“åº“: {github_url}")

        # å®‰å…¨éªŒè¯
        is_valid, error_msg = FormatDetector.validate_github_url(github_url)
        if not is_valid:
            error(f"GitHub URL å®‰å…¨éªŒè¯å¤±è´¥: {error_msg}")
            return False, target_dir

        # é¢„æ¸…ç†
        if target_dir.exists():
            info(f"ç›®æ ‡ç›®å½•å·²å­˜åœ¨ï¼Œæ¸…ç†: {target_dir}")
            try:
                subprocess.run(["rm", "-rf", str(target_dir)], capture_output=True, timeout=10)
            except:
                pass
            if target_dir.exists():
                warn(f"ç›®å½•æ¸…ç†å¤±è´¥ï¼Œä½¿ç”¨ shutil å¼ºåˆ¶é‡è¯•: {target_dir}")
                time.sleep(0.5)
                shutil.rmtree(target_dir, ignore_errors=False)

        # æ„å»º git å‘½ä»¤
        cmd = ["git"]
        if not get_ssl_verify():
            cmd.extend(["-c", "http.sslVerify=false"])
        cmd.extend(["-c", "core.longPaths=true"])

        # ç¯å¢ƒå˜é‡
        env = os.environ.copy()
        env["GIT_TERMINAL_PROMPT"] = "0"
        env["GIT_ASKPASS"] = "true"

        # å°è¯•åŠ é€Ÿå™¨
        proxies = get_git_proxies()
        for proxy_template in proxies:
            repo_path = github_url.replace("https://github.com/", "").replace("http://github.com/", "")
            proxy_url = proxy_template.replace("{repo}", repo_path)

            try:
                clone_cmd = cmd + ["clone", "--depth", "1", proxy_url, str(target_dir)]
                result = subprocess.run(
                    clone_cmd,
                    capture_output=True,
                    text=True,
                    encoding="utf-8",
                    errors="replace",
                    timeout=20,
                    env=env,
                )

                if result.returncode == 0:
                    success(f"å…‹éš†æˆåŠŸï¼ˆä½¿ç”¨åŠ é€Ÿå™¨ï¼‰: {target_dir}")
                    return True, target_dir
                else:
                    warn(f"åŠ é€Ÿå™¨å…‹éš†å¤±è´¥: {proxy_url}")
            except subprocess.TimeoutExpired:
                warn(f"åŠ é€Ÿå™¨è¶…æ—¶: {proxy_url}")
                continue
            except Exception as e:
                warn(f"åŠ é€Ÿå™¨å¼‚å¸¸: {e}")
                continue

        # å›é€€åˆ°åŸå§‹åœ°å€
        try:
            clone_cmd = cmd + ["clone", "--depth", "1", github_url, str(target_dir)]
            result = subprocess.run(
                clone_cmd,
                capture_output=True,
                text=True,
                encoding="utf-8",
                errors="replace",
                timeout=300,
                env=env,
            )

            if result.returncode == 0:
                success(f"å…‹éš†æˆåŠŸ: {target_dir}")
                return True, target_dir
            else:
                error(f"å…‹éš†å¤±è´¥: {result.stderr}")
                return False, target_dir

        except subprocess.TimeoutExpired:
            error("å…‹éš†è¶…æ—¶")
            return False, target_dir
        except Exception as e:
            error(f"å…‹éš†å¼‚å¸¸: {e}")
            return False, target_dir

    @staticmethod
    def _recursive_skill_scan(
        root_dir: Path,
        max_depth: int = 5,
        exclude_dirs: Optional[set] = None
    ) -> List[Dict[str, Path]]:
        """é€’å½’æ‰«ææ‰€æœ‰å­ç›®å½•ï¼ŒæŸ¥æ‰¾ SKILL.md æ–‡ä»¶"""
        if exclude_dirs is None:
            exclude_dirs = {"examples", "templates", "test", "tests", "docs", "reference", ".git", "node_modules", "__pycache__"}

        results = []

        def _scan_recursive(current_dir: Path, current_depth: int, rel_path: str = ""):
            if current_depth > max_depth:
                return

            try:
                for item in current_dir.iterdir():
                    if item.name in exclude_dirs or item.name.startswith('.'):
                        continue

                    if item.is_dir():
                        new_rel_path = f"{rel_path}/{item.name}" if rel_path else item.name

                        if (item / "SKILL.md").exists():
                            results.append({
                                "path": item,
                                "relative_path": new_rel_path
                            })
                            info(f"å‘ç°æ·±å±‚æŠ€èƒ½: {new_rel_path}")

                        _scan_recursive(item, current_depth + 1, new_rel_path)
            except (PermissionError, OSError):
                pass

        _scan_recursive(root_dir, 0)
        return results

    @staticmethod
    def extract_skills(repo_dir: Path, skill_name: Optional[str] = None) -> List[Path]:
        """ä»ä»“åº“ä¸­æå–æŠ€èƒ½ç›®å½•

        Args:
            repo_dir: ä»“åº“ç›®å½•
            skill_name: å¯é€‰ï¼Œä»…æå–æŒ‡å®šçš„æŠ€èƒ½å

        Returns:
            æŠ€èƒ½ç›®å½•åˆ—è¡¨
        """
        skill_dirs = []
        exclude_dirs = {"examples", "templates", "test", "tests", "docs", "reference"}

        # æ£€æµ‹ .skill åŒ…æ–‡ä»¶
        skill_packages = list(repo_dir.glob("*.skill"))
        if skill_packages:
            info(f"æ£€æµ‹åˆ°æŠ€èƒ½åŒ…æ–‡ä»¶: {skill_packages[0].name}")
            extract_dir = repo_dir.parent / f"{repo_dir.name}_extracted"
            extract_ok, extracted = SkillPackHandler.extract_pack(skill_packages[0], extract_dir)
            if extract_ok:
                skill_dirs.append(extracted)
            return skill_dirs

        # å¹³å°ä¼˜å…ˆçº§é…ç½®
        platform_priority = [
            repo_dir / ".agent" / "skills",
            repo_dir / ".claude" / "skills",
            repo_dir / "skills",
            repo_dir / ".claude-plugin",
            repo_dir / ".cursor" / "rules",
            repo_dir,
        ]

        # æ£€æµ‹ Claude Code ç‰ˆæœ¬
        claude_code_skills = []
        agent_skills_dir = repo_dir / ".agent" / "skills"
        if agent_skills_dir.exists():
            for item in agent_skills_dir.glob("*/SKILL.md"):
                if item.parent.name not in exclude_dirs:
                    claude_code_skills.append(item.parent)

        if claude_code_skills:
            return claude_code_skills

        # æŒ‰ä¼˜å…ˆçº§æ‰«æ
        for location in platform_priority[1:]:
            if location.exists() and location.is_dir():
                if location == repo_dir:
                    if (location / "SKILL.md").exists():
                        sub_skill_dirs = [
                            item.parent for item in repo_dir.glob("*/SKILL.md")
                            if item.parent.name not in exclude_dirs
                        ]
                        MULTI_SKILL_THRESHOLD = 3
                        if len(sub_skill_dirs) >= MULTI_SKILL_THRESHOLD:
                            info(f"æ£€æµ‹åˆ°å¤šæŠ€èƒ½å®¹å™¨: {repo_dir.name} (åŒ…å« {len(sub_skill_dirs)} ä¸ªå­æŠ€èƒ½)")
                            skill_dirs.extend(sub_skill_dirs)
                        else:
                            skill_dirs.append(location)
                        continue
                    else:
                        skills_dir = repo_dir / "skills"
                        if skills_dir.exists() and skills_dir.is_dir():
                            sub_skill_count = 0
                            sub_skill_candidates = []
                            for item in skills_dir.iterdir():
                                if item.is_dir() and (item / "SKILL.md").exists():
                                    sub_skill_candidates.append(item)
                                    sub_skill_count += 1
                            if sub_skill_count >= 1:
                                info(f"æ£€æµ‹åˆ° monorepo: skills/ ç›®å½•åŒ…å« {sub_skill_count} ä¸ªæŠ€èƒ½")
                                skill_dirs.extend(sub_skill_candidates)
                                continue

                        root_sub_skills = []
                        for item in repo_dir.iterdir():
                            if item.is_dir() and not item.name.startswith(".") and item.name not in exclude_dirs:
                                if (item / "SKILL.md").exists():
                                    root_sub_skills.append(item)
                        if len(root_sub_skills) >= 2:
                            info(f"æ£€æµ‹åˆ° monorepo: æ ¹ç›®å½•åŒ…å« {len(root_sub_skills)} ä¸ªå­æŠ€èƒ½")
                            skill_dirs.extend(root_sub_skills)
                            continue

                if location.name == "skills" and location.parent == repo_dir:
                    continue

                sub_skill_count = 0
                sub_skill_candidates = []
                for item in location.iterdir():
                    if item.is_dir() and item.name not in exclude_dirs:
                        if (item / "SKILL.md").exists():
                            sub_skill_count += 1
                            sub_skill_candidates.append(item)

                MULTI_SKILL_THRESHOLD = 3
                if sub_skill_count >= MULTI_SKILL_THRESHOLD:
                    info(f"æ£€æµ‹åˆ°å¤šæŠ€èƒ½å­ç›®å½•: {location.name} (åŒ…å« {sub_skill_count} ä¸ªå­æŠ€èƒ½)")
                    skill_dirs.extend(sub_skill_candidates)
                    continue

                for item in location.iterdir():
                    if item.is_dir() and (not item.name.startswith(".") or item.name == ".claude"):
                        if item.name in exclude_dirs:
                            continue
                        has_skill = (item / "SKILL.md").exists() or list(item.glob("*.md"))
                        if has_skill:
                            skill_dirs.append(item)

        # å›é€€æœºåˆ¶1ï¼šé€’å½’æ·±åº¦æ‰«æ
        if not skill_dirs:
            recursive_results = GitHubHandler._recursive_skill_scan(repo_dir, max_depth=5)
            if recursive_results:
                recursive_results.sort(key=lambda x: x["relative_path"].count('/'))
                skill_dirs = [r["path"] for r in recursive_results]
                info(f"é€’å½’æ‰«æ: å‘ç° {len(skill_dirs)} ä¸ªæ·±å±‚å­æŠ€èƒ½")

        # å›é€€æœºåˆ¶2ï¼šå•å±‚å­ç›®å½•
        if not skill_dirs:
            fallback_skills = []
            for item in repo_dir.iterdir():
                if item.is_dir() and not item.name.startswith(".") and item.name not in exclude_dirs:
                    if (item / "SKILL.md").exists():
                        fallback_skills.append(item)
            if fallback_skills:
                info(f"å›é€€æ£€æµ‹: å‘ç° {len(fallback_skills)} ä¸ªå­æŠ€èƒ½")
                skill_dirs.extend(fallback_skills)

        # æ ¹æ® skill_name è¿‡æ»¤
        if skill_name:
            normalized_target = skill_name.lower().replace('_', '-')
            filtered = []
            for skill_dir in skill_dirs:
                dir_name = skill_dir.name
                # æ”¯æŒç›´æ¥åŒ¹é…å’Œ _/- å˜ä½“åŒ¹é…
                if dir_name.lower() == skill_name.lower() or \
                   dir_name.lower().replace('_', '-') == normalized_target or \
                   dir_name.lower().replace('-', '_') == normalized_target:
                    filtered.append(skill_dir)
                    info(f"è¿‡æ»¤åŒ¹é…: {dir_name}")
            if filtered:
                info(f"å·²è¿‡æ»¤: {len(skill_dirs)} -> {len(filtered)} ä¸ªæŠ€èƒ½")
                skill_dirs = filtered
            else:
                warn(f"æœªæ‰¾åˆ°åŒ¹é…çš„æŠ€èƒ½: {skill_name}ï¼Œè¿”å›å…¨éƒ¨æŠ€èƒ½")

        return skill_dirs

# =============================================================================
# å…±äº«é€»è¾‘
# =============================================================================

def _extract_repo_from_url(github_url: str) -> Optional[str]:
    """ä» GitHub URL æå– user/repo æ ¼å¼"""
    parsed = urlparse(github_url)
    if "github.com" not in parsed.netloc:
        return None
    path_parts = parsed.path.strip('/').split('/')
    if len(path_parts) >= 2:
        return f"{path_parts[0]}/{path_parts[1]}"
    return None

def _dual_path_skill_check(github_url: str) -> Tuple[bool, str, List[str]]:
    """ç»Ÿä¸€çš„æŠ€èƒ½åˆ¤å®š"""
    parsed = urlparse(github_url)
    if "github.com" not in parsed.netloc:
        return True, "é GitHub URLï¼Œè·³è¿‡é¢„æ£€", ["fallback"]

    repo = _extract_repo_from_url(github_url)
    if not repo:
        return True, "æ— æ³•è§£æä»“åº“ï¼Œè·³è¿‡é¢„æ£€", ["fallback"]

    analyzer = RemoteSkillAnalyzer(repo)
    is_skill, reason = analyzer.check_is_skill_repo()

    if is_skill is True:
        return True, reason, ["analyzer"]
    elif is_skill is False:
        return False, reason, []
    else:
        return True, reason, ["fallback"]

def _process_github_source(
    github_url: str,
    skill_name: Optional[str] = None,
    use_cache: bool = True,
    force_refresh: bool = False,
    temp_dir: Optional[Path] = None
) -> Tuple[List[Path], str]:
    """
    å¤„ç† GitHub æºï¼ˆå…‹éš† + æå–æŠ€èƒ½ï¼‰

    Returns:
        (æŠ€èƒ½ç›®å½•åˆ—è¡¨, æ¶ˆæ¯)
    """
    if temp_dir is None:
        temp_dir = TEMP_DIR
        temp_dir.mkdir(parents=True, exist_ok=True)

    # 1. æŠ€èƒ½ä»“åº“é¢„æ£€
    should_proceed, reason, sources = _dual_path_skill_check(github_url)
    if not should_proceed:
        return [], reason

    # 2. å­æŠ€èƒ½é¢„æ£€ï¼ˆä»…å½“æŒ‡å®šäº†å­æŠ€èƒ½åï¼‰
    if skill_name:
        repo = _extract_repo_from_url(github_url)
        if repo:
            try:
                analyzer = RemoteSkillAnalyzer(repo)
                if not analyzer._verify_single_skill(skill_name):
                    warn(f"å­æŠ€èƒ½é¢„æ£€å¤±è´¥: {skill_name}")
            except Exception:
                pass

    # 3. ç¼“å­˜æœºåˆ¶
    if use_cache:
        cache_ok, cache_dir, cache_msg = RepoCacheManager.get_or_clone(
            github_url,
            force_refresh=force_refresh
        )
        if cache_ok and cache_dir:
            scan_dir = cache_dir
        else:
            return [], cache_msg
    else:
        clone_ok, repo_dir = GitHubHandler.clone_repo(github_url, temp_dir / "repo")
        if not clone_ok:
            return [], "ä»“åº“ä¸å­˜åœ¨æˆ–è·¯å¾„é”™è¯¯\næç¤º: æŸ¥çœ‹ docs/skills-mapping.md ç¡®è®¤æ­£ç¡®çš„ä»“åº“è·¯å¾„"
        scan_dir = repo_dir

    # 4. å¤„ç†å­è·¯å¾„
    parsed = urlparse(github_url)
    if 'tree' in parsed.path:
        path_parts = parsed.path.strip('/').split('/')
        tree_idx = path_parts.index('tree')
        if len(path_parts) > tree_idx + 2:
            subpath = Path(scan_dir) / '/'.join(path_parts[tree_idx + 2:])
            if subpath.exists():
                scan_dir = subpath
            else:
                return [], f"å­è·¯å¾„ä¸å­˜åœ¨: {subpath}"

    # 5. æå–æŠ€èƒ½ï¼ˆåº”ç”¨ skill_name è¿‡æ»¤ï¼‰
    skill_dirs = GitHubHandler.extract_skills(scan_dir, skill_name)
    if not skill_dirs:
        return [], f"æœªæ‰¾åˆ°æŠ€èƒ½ï¼Œè¯·ç¡®è®¤ä»“åº“æ˜¯å¦ä¸ºæŠ€èƒ½ä»“åº“"

    return skill_dirs, f"æˆåŠŸè·å– {len(skill_dirs)} ä¸ªæŠ€èƒ½"

# =============================================================================
# CLI å…¥å£
# =============================================================================

def main():
    """CLI å…¥å£"""
    parser = argparse.ArgumentParser(
        description="clone_manager.py - GitHub ä»“åº“å…‹éš†ç®¡ç†å™¨",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # å…‹éš†ä»“åº“åˆ°ç¼“å­˜
  python bin/clone_manager.py clone https://github.com/user/repo

  # å…‹éš†å¹¶æŒ‡å®šå­æŠ€èƒ½
  python bin/clone_manager.py clone https://github.com/user/repo --skill my-skill

  # åˆ—å‡ºç¼“å­˜
  python bin/clone_manager.py list-cache

  # æ¸…ç†ç¼“å­˜
  python bin/clone_manager.py clear-cache
        """
    )

    subparsers = parser.add_subparsers(dest="command", help="å¯ç”¨å‘½ä»¤")

    # clone å‘½ä»¤
    clone_parser = subparsers.add_parser("clone", help="å…‹éš† GitHub ä»“åº“")
    clone_parser.add_argument("url", help="GitHub ä»“åº“ URL")
    clone_parser.add_argument("--skill", help="æŒ‡å®šå­æŠ€èƒ½åç§°")
    clone_parser.add_argument("--force", action="store_true", help="å¼ºåˆ¶åˆ·æ–°ç¼“å­˜")
    clone_parser.add_argument("--no-cache", action="store_true", help="ä¸ä½¿ç”¨ç¼“å­˜")

    # list-cache å‘½ä»¤
    subparsers.add_parser("list-cache", help="åˆ—å‡ºæ‰€æœ‰ç¼“å­˜")

    # clear-cache å‘½ä»¤
    clear_parser = subparsers.add_parser("clear-cache", help="æ¸…ç†ç¼“å­˜")
    clear_parser.add_argument("--older-than", type=int, help="åªæ¸…ç†è¶…è¿‡æŒ‡å®šå°æ—¶æ•°çš„ç¼“å­˜")

    args = parser.parse_args()

    if not args.command:
        parser.print_help()
        return 1

    if args.command == "clone":
        skill_dirs, msg = _process_github_source(
            args.url,
            skill_name=getattr(args, 'skill', None),
            use_cache=not getattr(args, 'no_cache', False),
            force_refresh=getattr(args, 'force', False)
        )

        if not skill_dirs:
            error(msg)
            return 1

        success(msg)
        print("æŠ€èƒ½ç›®å½•:")
        for skill_dir in skill_dirs:
            print(f"  - {skill_dir}")
        return 0

    elif args.command == "list-cache":
        caches = RepoCacheManager.list_cache()
        if not caches:
            print("æ²¡æœ‰ç¼“å­˜")
            return 0

        print(f"å…±æœ‰ {len(caches)} ä¸ªç¼“å­˜:")
        for cache in caches:
            print(f"  - {cache['url']}")
            print(f"    ç¼“å­˜æ—¶é—´: {cache['cached_at']}")
            print(f"    å¤§å°: {cache['size_mb']} MB")
        return 0

    elif args.command == "clear-cache":
        result = RepoCacheManager.clear_cache(
            older_than_hours=getattr(args, 'older_than', None)
        )
        print(f"æ¸…ç†å®Œæˆ: {result['cleared']} ä¸ªå·²åˆ é™¤, {result['kept']} ä¸ªå·²ä¿ç•™")
        return 0

    return 0


if __name__ == "__main__":
    sys.exit(main())
